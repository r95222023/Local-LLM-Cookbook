{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8723a4fd-8887-4b96-9e5a-ee7b6b9cb336",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from langchain import hub\n",
    "from langchain_experimental.llms.ollama_functions import OllamaFunctions\n",
    "# from langgraph.prebuilt import create_react_agent\n",
    "from langchain.globals import set_verbose, set_debug\n",
    "from langchain.agents import AgentExecutor, create_react_agent\n",
    "from langgraph.constants import Send\n",
    "from langgraph.graph import END, StateGraph, START\n",
    "from langchain.globals import set_verbose, set_debug\n",
    "from typing import List, TypedDict\n",
    "\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "from langchain_experimental.llms.ollama_functions import OllamaFunctions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5da7f20e-65dc-4565-97a5-c5083ddfe4fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup as Soup\n",
    "from langchain_community.document_loaders.recursive_url_loader import RecursiveUrlLoader\n",
    "\n",
    "# retriever docs\n",
    "url = \"https://python.langchain.com/v0.2/docs/tutorials/retrievers/\"\n",
    "loader = RecursiveUrlLoader(\n",
    "    url=url, max_depth=20, extractor=lambda x: Soup(x, \"html.parser\").text\n",
    ")\n",
    "docs = loader.load()\n",
    "\n",
    "# Sort the list based on the URLs and get the text\n",
    "d_sorted = sorted(docs, key=lambda x: x.metadata[\"source\"])\n",
    "d_reversed = list(reversed(d_sorted))\n",
    "concatenated_content = \"\\n\\n\\n --- \\n\\n\\n\".join(\n",
    "    [doc.page_content for doc in d_reversed]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c8f3bff2-9070-4aeb-8d94-a8f602c6906f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_community.chat_models import ChatOllama\n",
    "llm = ChatOllama(model=\"llama3:instruct\", temperature=0.1)\n",
    "\n",
    "# llm = OllamaFunctions(model=\"llama3:instruct\", format=\"json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "05f960eb-fc5e-40d7-bf71-442685da719b",
   "metadata": {},
   "outputs": [],
   "source": [
    "solution_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\",\"\"\" You are a coding expert with expertise in Python, LangChain and LangGraph. \n",
    "        \\nHere is the user question: \\n {task}\n",
    "        \\nHere is the documentation for a library:  \\n ------- \\n  {context} \\n ------- \\n Answer the user question based on the \\n \n",
    "        above provided documentation. Ensure any code you provide can be executed with all required imports and variables defined.\n",
    "        \\n\\n[INSERT YOUR CODE HERE]\n",
    "        \"\"\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "solution_prompt_reflect = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"\"\" You are a coding expert with expertise in Python, LangChain and LangGraph. \\n \n",
    "    Here is the documentation for a library:  \\n ------- \\n  {context} \\n ------- \\n You tried to answer the user question based on the \\n \n",
    "    above provided documentation, but your last attempt was not successful! Here is the reflections for your last attempt \n",
    "    \\n ------- \\n  {reflections} \\n ------- \\n\n",
    "    \\n Answer the user question again. Fix the code and ensure any code you provide can be executed with all required imports and variables \\n\n",
    "    defined.\\n Here is the user question:\"\"\",\n",
    "        ),\n",
    "        (\"placeholder\", \"{task}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "description_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"\"\" You are a coding expert with expertise in Python, LangChain and LangGraph. \\n \n",
    "    The solution you provided in the last step is\n",
    "    \\n ------- \\n  {solution} \\n ------- \\n \n",
    "    \\n\\n Here is the user question:\n",
    "    \\n\\n{task}\n",
    "    \\n\n",
    "    \\nGive description for this code, and ensure that the description is clear and concise.\n",
    "    \\n[INSERT DESCRIPTION HERE]\"\"\",\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "imports_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"\"\" You are a coding expert with expertise in Python, LangChain and LangGraph. \\n \n",
    "    The solution you provided in the last step is\n",
    "    \\n ------- \\n  {solution} \\n ------- \\n \n",
    "    \\n Extract the imports part for this code.\n",
    "    \\nEXAMPLE:\n",
    "    \\n\\n\n",
    "    ```\n",
    "    \\nfrom dotenv import load_dotenv\n",
    "    \\nfrom langchain_community.tools.tavily_search import TavilySearchResults\n",
    "    \\nfrom langchain import hub\n",
    "    ```\n",
    "    \\n\\n[INSERT IMPORTS HERE]\n",
    "    \"\"\",\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "code_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"\"\" You are a coding expert with expertise in Python, LangChain and LangGraph. \\n \n",
    "    The solution you provided in the last step is\n",
    "    \\n ------- \\n  {solution} \\n ------- \\n \n",
    "    \\n Extract the code block without imports part for this code, and ensure any code you provide can be executed.\n",
    "    \\nThe output should be formatted as follows:\n",
    "    \\nEXAMPLE: \n",
    "    \\n\\n\n",
    "    ```\n",
    "    \\nexpt_llm = \"claude-3-opus-20240229\"\n",
    "    \\nllm = ChatAnthropic(\n",
    "    \\nmodel=expt_llm,\n",
    "    \\ndefault_headers=None,)\n",
    "\n",
    "    \\nstructured_llm_claude = llm.with_structured_output(code, include_raw=True)\n",
    "    \\n\n",
    "    ```\n",
    "    \\n\\n[INSERT CODE HERE]\"\"\",\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "reflect_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"\"\" You are a coding expert with expertise in Python, LangChain and LangGraph. \\n \n",
    "    The solution you provided in the last step is\n",
    "    \\n ------- \\n  {solution} \\n ------- \\n \n",
    "    \\n Extract the code block for this code, and ensure any code you provide can be executed.\"\"\",\n",
    "        ),\n",
    "        (\"placeholder\", \"{messages}\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a9668969-d755-431a-8281-688d4707f1b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphState(TypedDict):\n",
    "    solution: str\n",
    "    description: str\n",
    "    # prefix: str\n",
    "    imports: str\n",
    "    code: str\n",
    "    iterations: int\n",
    "    error: str\n",
    "    task: str\n",
    "    messages: List\n",
    "    reflections: str\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "172399bc-b8b7-42ef-9310-fc03c2e51401",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def extract_code(text):\n",
    "    pattern = r\"```(.*?)```\"\n",
    "    matches = re.findall(pattern, text, re.DOTALL)\n",
    "    return matches[0]\n",
    "\n",
    "class FormattedOutput:\n",
    "    def __init__(self, output):\n",
    "        self.description = output['description']\n",
    "        self.imports = extract_code(output['imports'])\n",
    "        self.code = extract_code(output['code'])\n",
    "        self.solution = extract_code(output['solution'])\n",
    "\n",
    "def parse_arg(ai_msg, arg):\n",
    "    pattern_string = f'{arg}=([^\\s,)]+)'\n",
    "    regex = re.compile(pattern_string) \n",
    "    # pattern = r'location=([^\\s,)]+)'\n",
    "    # match = re.search(pattern, ai_msg.content)\n",
    "    match = regex.search(ai_msg.content)\n",
    "    if match:\n",
    "        try:\n",
    "            return eval(match.group(1))\n",
    "        except Exception as e:\n",
    "            return match.group(1) or \"\"\n",
    "\n",
    "    \n",
    "class CodeAssistant:\n",
    "    def __init__(self, llm, context=\"\", max_iterations=5, reflect=False, check_code=False):\n",
    "        self.compiled = False\n",
    "        self.workflow = None\n",
    "        self.context = context\n",
    "        \n",
    "        self.llm = llm\n",
    "        self.max_iterations = max_iterations\n",
    "        self.reflect = reflect\n",
    "        self.check_code = check_code\n",
    "        # self.retry_cahin = retry_prompt | self.llm # {\"task\", \"reflections\", \"tool_calls\"}\n",
    "\n",
    "        self.solution_gen_chain = solution_prompt| llm\n",
    "        self.solution_gen_chain_reflect = solution_prompt_reflect | llm\n",
    "        self.description_gen_chain = description_prompt | llm\n",
    "        self.imports_gen_chain = imports_prompt | llm\n",
    "        self.code_gen_chain = code_prompt | llm\n",
    "        self.workflow = StateGraph(GraphState)\n",
    "\n",
    "    def invoke(self, task):\n",
    "        if not self.compiled:\n",
    "            app = self.compile()\n",
    "        else:\n",
    "            app = self.workflow\n",
    "        return app.invoke({\"context\":concatenated_content, \"task\": task, \"iterations\": 0, \"messages\":[]})\n",
    "\n",
    "    def compile(self):\n",
    "        workflow = StateGraph(GraphState)\n",
    "        workflow.add_node(\"generate_node\", self.generation_node)\n",
    "        workflow.add_node(\"description_node\", self.description_node)\n",
    "        workflow.add_node(\"imports_node\", self.imports_node)\n",
    "        workflow.add_node(\"code_node\", self.code_node)\n",
    "        # workflow.add_node(\"reflect\", reflect)\n",
    "        \n",
    "        # Build graph\n",
    "        workflow.add_edge(START, \"generate_node\")\n",
    "        workflow.add_edge(\"generate_node\", \"description_node\")\n",
    "        workflow.add_edge(\"description_node\", \"imports_node\")\n",
    "        workflow.add_edge(\"imports_node\", \"code_node\")\n",
    "        workflow.add_edge(\"code_node\", END)\n",
    "        # workflow.add_edge(\"code_node\", \"code_check_node\")\n",
    "        # workflow.add_conditional_edges(\n",
    "        #     \"code_check_node\",\n",
    "        #     decide_to_finish,\n",
    "        #     {\n",
    "        #         \"end\": END,\n",
    "        #         \"reflect\": \"reflect\",\n",
    "        #         \"retry\": \"description_node\",\n",
    "        #     },\n",
    "        # )\n",
    "        # workflow.add_edge(\"reflect\", \"generate_node\")\n",
    "\n",
    "        \n",
    "        self.workflow = workflow.compile()\n",
    "        self.compiled = True\n",
    "        return self.workflow\n",
    "        \n",
    "    def generation_node(self, state: GraphState):\n",
    "        \"\"\"\n",
    "        Generate a code solution\n",
    "    \n",
    "        Args:\n",
    "            state (dict): The current graph state\n",
    "    \n",
    "        Returns:\n",
    "            state (dict): New key added to state, generation\n",
    "        \"\"\"\n",
    "    \n",
    "        print(\"---GENERATING SOLUTION---\")\n",
    "    \n",
    "        # State\n",
    "        task = state[\"task\"]\n",
    "        iterations = state[\"iterations\"]\n",
    "        reflections = state[\"reflections\"]\n",
    "        error = state[\"error\"]\n",
    "        messages = state[\"messages\"]\n",
    "    \n",
    "        # Increment\n",
    "        iterations = iterations + 1\n",
    "        \n",
    "        # We have been routed back to generation with an error\n",
    "        if error == \"yes\":\n",
    "            messages += [\n",
    "                (\n",
    "                    \"user\",\n",
    "                    \"Now, try again. Invoke the code tool to structure the output with a prefix, imports, and code block:\",\n",
    "                )\n",
    "            ]\n",
    "            # Solution\n",
    "            code_solution = self.solution_gen_chain_reflect.invoke({\"context\":self.context, \"task\": task, \"reflections\": reflections}).content\n",
    "        else:\n",
    "            code_solution = self.solution_gen_chain.invoke({\"context\":self.context, \"task\": task}).content\n",
    "        \n",
    "        return {**state, \"solution\": code_solution, \"iterations\": iterations, \"messages\": messages}\n",
    "\n",
    "\n",
    "    def description_node(self, state: GraphState):\n",
    "        \"\"\"\n",
    "        Generate a code description\n",
    "    \n",
    "        Args:\n",
    "            state (dict): The current graph state\n",
    "    \n",
    "        Returns:\n",
    "            state (dict): New key added to state, generation\n",
    "        \"\"\"\n",
    "    \n",
    "        print(\"---GENERATING CODE DESCRIPTION---\")\n",
    "    \n",
    "        # State\n",
    "        task = state[\"task\"]\n",
    "        solution = state[\"solution\"]\n",
    "    \n",
    "        # Description\n",
    "        code_description = self.description_gen_chain.invoke(\n",
    "            {\"solution\": solution, \"task\": task}\n",
    "        ).content\n",
    "    \n",
    "        return {**state, \"description\": code_description}\n",
    "    \n",
    "    \n",
    "    def imports_node(self, state: GraphState):\n",
    "        \"\"\"\n",
    "        Generate a code imports\n",
    "    \n",
    "        Args:\n",
    "            state (dict): The current graph state\n",
    "    \n",
    "        Returns:\n",
    "            state (dict): New key added to state, generation\n",
    "        \"\"\"\n",
    "    \n",
    "        print(\"---GENERATING IMPORTS BLOCK---\")\n",
    "    \n",
    "        # State\n",
    "        task = state[\"task\"]\n",
    "        solution = state[\"solution\"]\n",
    "    \n",
    "        # Description\n",
    "        code_imports = self.imports_gen_chain.invoke(\n",
    "            {\"solution\": solution}\n",
    "        ).content\n",
    "    \n",
    "        return {**state, \"imports\": code_imports}\n",
    "    \n",
    "    \n",
    "    def code_node(self, state: GraphState):\n",
    "        \"\"\"\n",
    "        Generate a code parts without imports\n",
    "    \n",
    "        Args:\n",
    "            state (dict): The current graph state\n",
    "    \n",
    "        Returns:\n",
    "            state (dict): New key added to state, generation\n",
    "        \"\"\"\n",
    "    \n",
    "        print(\"---GENERATING CODE BLOCK---\")\n",
    "    \n",
    "        # State\n",
    "        task = state[\"task\"]\n",
    "        solution = state[\"solution\"]\n",
    "    \n",
    "        # Description\n",
    "        code_block = self.code_gen_chain.invoke(\n",
    "            {\"solution\": solution}\n",
    "        ).content\n",
    "    \n",
    "        return {**state, \"code\": code_block}\n",
    "    \n",
    "    \n",
    "    def code_check_node(self, state: GraphState):\n",
    "        \"\"\"\n",
    "        Check code\n",
    "    \n",
    "        Args:\n",
    "            state (dict): The current graph state\n",
    "    \n",
    "        Returns:\n",
    "            state (dict): New key added to state, error\n",
    "        \"\"\"\n",
    "        if not self.check_code:\n",
    "            return {\n",
    "                **state,\n",
    "                \"error\": \"no\",\n",
    "            }\n",
    "            \n",
    "        print(\"---CHECKING CODE---\")\n",
    "    \n",
    "        # State\n",
    "        imports = state[\"imports\"]\n",
    "        code = state[\"code\"]\n",
    "    \n",
    "        # Check imports\n",
    "        try:\n",
    "            exec(imports)\n",
    "        except Exception as e:\n",
    "            print(\"---CODE IMPORT CHECK: FAILED---\")\n",
    "            error_message = [(\"user\", f\"Your solution failed the import test: {e}\")]\n",
    "            messages += error_message\n",
    "            return {\n",
    "                **state,\n",
    "                \"messages\": messages,\n",
    "                \"error\": \"yes\",\n",
    "            }\n",
    "    \n",
    "        # Check execution\n",
    "        try:\n",
    "            exec(imports + \"\\n\" + code)\n",
    "        except Exception as e:\n",
    "            print(\"---CODE BLOCK CHECK: FAILED---\")\n",
    "            error_message = [(\"user\", f\"Your solution failed the code execution test: {e}\")]\n",
    "            messages += error_message\n",
    "            return {\n",
    "                **state,\n",
    "                \"messages\": messages,\n",
    "                \"error\": \"yes\",\n",
    "            }\n",
    "    \n",
    "        # No errors\n",
    "        print(\"---NO CODE TEST FAILURES---\")\n",
    "        return {\n",
    "            **state,\n",
    "            \"error\": \"no\",\n",
    "        }\n",
    "    \n",
    "    \n",
    "    def reflect_node(self, state: GraphState):\n",
    "        \"\"\"\n",
    "        Reflect on errors\n",
    "    \n",
    "        Args:\n",
    "            state (dict): The current graph state\n",
    "    \n",
    "        Returns:\n",
    "            state (dict): New key added to state, generation\n",
    "        \"\"\"\n",
    "    \n",
    "        print(\"---GENERATING CODE SOLUTION---\")\n",
    "    \n",
    "        # State\n",
    "        messages = state[\"messages\"]\n",
    "        solution = state[\"solution\"]\n",
    "        task = state[\"task\"]\n",
    "    \n",
    "        # Prompt reflection\n",
    "    \n",
    "        # Add reflection\n",
    "        reflections = reflect_gen_chain.invoke(\n",
    "            {\"task\": task, \"solution\": solution, \"messages\": messages}\n",
    "        ).content\n",
    "        messages += [(\"assistant\", f\"Here are reflections on the error: {reflections}\")]\n",
    "        return {**state, \"reflections\": reflections, \"messages\": messages}\n",
    "    \n",
    "    \n",
    "    ### Edges\n",
    "    \n",
    "    \n",
    "    def decide_to_finish(self, state: GraphState):\n",
    "        \"\"\"\n",
    "        Determines whether to finish.\n",
    "    \n",
    "        Args:\n",
    "            state (dict): The current graph state\n",
    "    \n",
    "        Returns:\n",
    "            str: Next node to call\n",
    "        \"\"\"\n",
    "        error = state[\"error\"]\n",
    "        iterations = state[\"iterations\"]\n",
    "    \n",
    "        if error == \"no\" or iterations == max_iterations:\n",
    "            print(\"---DECISION: FINISH---\")\n",
    "            return \"end\"\n",
    "        else:\n",
    "            print(\"---DECISION: RE-TRY SOLUTION---\")\n",
    "            if self.reflect:\n",
    "                return \"reflect\"\n",
    "            else:\n",
    "                return \"retry\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "43eeec70-38ca-474c-867b-415aa9428d35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---GENERATING SOLUTION---\n",
      "---GENERATING CODE DESCRIPTION---\n",
      "---GENERATING IMPORTS BLOCK---\n",
      "---GENERATING CODE BLOCK---\n"
     ]
    }
   ],
   "source": [
    "app = CodeAssistant(llm, context=concatenated_content)\n",
    "results = app.invoke(\"Write me a simple python code for retriever\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5339e779-7982-4b60-a758-b4b3e1b55fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# workflow = StateGraph(CodeState)\n",
    "\n",
    "# # Define the nodes\n",
    "# workflow.add_node(\"generate_node\", generation_node)\n",
    "# workflow.add_node(\"description_node\", description_node)\n",
    "# workflow.add_node(\"imports_node\", imports_node)\n",
    "# workflow.add_node(\"code_node\", code_node)\n",
    "# # workflow.add_node(\"reflect\", reflect)\n",
    "\n",
    "# # Build graph\n",
    "# workflow.add_edge(START, \"generate_node\")\n",
    "# workflow.add_edge(\"generate_node\", \"description_node\")\n",
    "# workflow.add_edge(\"description_node\", \"imports_node\")\n",
    "# workflow.add_edge(\"imports_node\", \"code_node\")\n",
    "# workflow.add_edge(\"code_node\", END)\n",
    "# # workflow.add_edge(\"code\", \"reflect\")\n",
    "# # workflow.add_conditional_edges(\n",
    "# #     \"check_code\",\n",
    "# #     decide_to_finish,\n",
    "# #     {\n",
    "# #         \"end\": END,\n",
    "# #         \"reflect\": \"reflect\",\n",
    "# #         \"description\": \"description\",\n",
    "# #     },\n",
    "# # )\n",
    "# # workflow.add_edge(\"reflect\", \"generate\")\n",
    "# app = workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "36560dbd-cc5d-45ca-b229-b54b4147d61e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is an example of how to combine a given question with retrieved context into a prompt for a LLM:\n",
      "\n",
      "```python\n",
      "from langchain_openai import ChatOpenAI\n",
      "import getpass\n",
      "import os\n",
      "\n",
      "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass()\n",
      "llm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\")\n",
      "\n",
      "question = \"What is the best way to learn Python?\"\n",
      "context = \"\"\"Python is a high-level, interpreted programming language that is easy to learn and understand. It has a vast number of libraries and frameworks that make it suitable for various tasks such as web development, data analysis, artificial intelligence, etc.\n",
      "\n",
      "Here are some tips on how to learn Python:\n",
      "\n",
      "1. Start with the basics: Understand the syntax and basic concepts of Python.\n",
      "2. Practice coding: The best way to learn any programming language is by writing code.\n",
      "3. Use online resources: There are many online resources available that can help you learn Python, such as tutorials, videos, and documentation.\n",
      "\n",
      "Remember, learning a programming language takes time and practice. Don't get discouraged if you don't understand something at first. Keep practicing and you will eventually become proficient in Python.\"\"\"\n",
      "\n",
      "prompt = f\"Answer this question using the provided context only.{question}Context:{context}\"\n",
      "\n",
      "llm_response = llm.run(prompt)\n",
      "\n",
      "print(llm_response)\n",
      "```\n",
      "\n",
      "This code uses the `ChatOpenAI` class from the `langchain_openai` library to create an instance of the LLM. It then defines a question and some context, combines them into a prompt, and runs the prompt through the LLM using the `run()` method. The response from the LLM is then printed.\n",
      "\n",
      "You can replace the `question` and `context` variables with your own values to create different prompts for the LLM.\n"
     ]
    }
   ],
   "source": [
    "print(results['solution'])\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "db642fff-463c-49b4-bfa9-73d15cc86a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "formatted_output = FormattedOutput(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "c7bd35da-fc99-494e-8a79-6c234c04313d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is the code block without imports:\n",
      "\n",
      "```\n",
      "llm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\")\n",
      "question = \"What is the best way to learn Python?\"\n",
      "context = \"\"\"Python is a high-level, interpreted programming language that is easy to learn and understand. It has a vast number of libraries and frameworks that make it suitable for various tasks such as web development, data analysis, machine learning, etc.\n",
      "\n",
      "Some popular resources to learn Python include:\n",
      "\n",
      "* Codecademy's Python Course\n",
      "* Python.org (official Python documentation)\n",
      "* W3Schools' Python Tutorial\n",
      "* Udemy's Python courses\n",
      "\n",
      "Additionally, you can also try practicing with online platforms like LeetCode, HackerRank, and Codewars.\n",
      "\"\"\"\n",
      "prompt = f\"Answer this question using the provided context only.{question}Context:{context}\"\n",
      "llm(prompt)\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "print(results['code'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "fb3b138d-b080-4d0e-8483-523aaad18b3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "llm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\")\n",
      "question = \"What is the best way to learn Python?\"\n",
      "context = \"\"\"Python is a high-level, interpreted programming language that is easy to learn and understand. It has a vast number of libraries and frameworks that make it suitable for various tasks such as web development, data analysis, machine learning, etc.\n",
      "\n",
      "Some popular resources to learn Python include:\n",
      "\n",
      "* Codecademy's Python Course\n",
      "* Python.org (official Python documentation)\n",
      "* W3Schools' Python Tutorial\n",
      "* Udemy's Python courses\n",
      "\n",
      "Additionally, you can also try practicing with online platforms like LeetCode, HackerRank, and Codewars.\n",
      "\"\"\"\n",
      "prompt = f\"Answer this question using the provided context only.{question}Context:{context}\"\n",
      "llm(prompt)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(formatted_output.code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "224a1cd2-ee25-4c93-97f0-9a29d3683056",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (llama3)",
   "language": "python",
   "name": "llama3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
