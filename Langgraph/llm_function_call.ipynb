{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c6d5ab78",
   "metadata": {},
   "source": [
    "# LLM Function Calling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a3cc54c",
   "metadata": {},
   "source": [
    "ToolLlm is designed for robust tool-calling functionality. If an error occurs during the execution of a tool, the LLM model will be asked to carefully examine each generated argument and attempt to fix the problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8723a4fd-8887-4b96-9e5a-ee7b6b9cb336",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from langchain import hub\n",
    "from langchain_experimental.llms.ollama_functions import OllamaFunctions\n",
    "# from langgraph.prebuilt import create_react_agent\n",
    "from langchain.globals import set_verbose, set_debug\n",
    "from langchain.agents import AgentExecutor, create_react_agent\n",
    "from langgraph.constants import Send\n",
    "from langgraph.graph import END, StateGraph, START\n",
    "from langchain.globals import set_verbose, set_debug\n",
    "from typing import List, TypedDict, Literal\n",
    "import json\n",
    "from langchain_core.tools import tool\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "from langchain_experimental.llms.ollama_functions import OllamaFunctions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c5c8bfb",
   "metadata": {},
   "source": [
    "## Initialize Local LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c8f3bff2-9070-4aeb-8d94-a8f602c6906f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.chat_models import ChatOllama\n",
    "llm = ChatOllama(model=\"llama3.1\", temperature=0.1)\n",
    "\n",
    "llm_function = OllamaFunctions(model=\"llama3.1\", format=\"json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c64fdc5",
   "metadata": {},
   "source": [
    "## Define Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "128f4cd7-bc4a-4c68-b8ce-68741ddd2188",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def get_weather(city: Literal[\"nyc\", \"sf\"]):\n",
    "    \"\"\"Use this to get weather information.\"\"\"\n",
    "    if city == \"nyc\":\n",
    "        return \"cloudy, 30 degrees celcius\"\n",
    "    elif city == \"sf\":\n",
    "        return \"sunny, 30 degrees celcius\"\n",
    "    else:\n",
    "        raise AssertionError(\"Unknown city\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5145f5b3-86f0-4fc1-9916-c1cc40aa9692",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [get_weather]\n",
    "available_tools = {'get_weather': get_weather}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cd8b1b7",
   "metadata": {},
   "source": [
    "## Prompts\n",
    "Modify prompts to achieve desired results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "05f960eb-fc5e-40d7-bf71-442685da719b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix_tool_args_prompt is used to fix function calling issues such as missing argument, wrong type argument etc.\n",
    "fix_tool_args_prompt = ChatPromptTemplate.from_messages( # {\"tool_name\", \"reflections\", \"arg\", \"tool_description\", args}\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"\"\"You are an AI assistant equipped with various tools to help answer questions and solve problems. \n",
    "            \\nYou have tried to use {tool_name}, but errors occurred. The reflections and tool informations are shown below:\n",
    "            \\nTool Description:\n",
    "            \\n\\n```\n",
    "            \\n{tool_description}\n",
    "            \\n```\n",
    "            \\n\n",
    "            \\nTool Args Schema:\n",
    "            \\n\\n```\n",
    "            \\n{args}\n",
    "            \\n```\n",
    "            \\n\n",
    "            \\nReflections:\n",
    "            \\n\\n```\n",
    "            \\n{reflections}\n",
    "            \\n```\n",
    "            \\n\n",
    "            \\nWhat is correct {arg} parameter needed to resolve the issue? Only give the best one.\n",
    "            \\nThe answer should follow the following format.\n",
    "            \\n\\nCorrect function call:\n",
    "            \\n\\n```\n",
    "            \\nmy_function(arg_1=fixed_arg1, arg_2=fixed_arg2)\n",
    "            \\n```\n",
    "            \\n\n",
    "            \\n[INSERT YOUR ANSWER HERE]\n",
    "            \"\"\",\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "# Modify finish_prompt to ensure the LLM-generated answer meets the required format.\n",
    "finish_prompt = ChatPromptTemplate.from_messages( # {\"task\", \"tool_outputs\"}\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"\"\"You are an AI assistant equipped with various tools to help answer questions and solve problems.\n",
    "            \\nYou have chosen to use the tools with outputs in JSON format shown below, \n",
    "            where the keys are the tool names and the value are their outputs. \n",
    "            \\n\\n```\n",
    "            \\n{tool_outputs}\n",
    "            \\n\\n```\n",
    "            \\n\n",
    "            Use tool outputs to answer the user's question. Do not make up any data.\\n Here is the user question:\"\"\",\n",
    "        ),\n",
    "        (\"placeholder\", \"{task}\"),\n",
    "    ]\n",
    ")\n",
    "# If the LLM-generated answer does not have tool calls, this prompt will be used to generate the final answer.\n",
    "finish_wo_calls_prompt = ChatPromptTemplate.from_messages( # {\"task\", \"tool_outputs\"}\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"\"\"You are an helpful AI assistant who is good at answering questions and solve problems.\n",
    "            \\n Here is the user question:\"\"\",\n",
    "        ),\n",
    "        (\"placeholder\", \"{task}\"),\n",
    "    ]\n",
    ")\n",
    "#  reflect_prompt is designed to make the LLM review and assess its previous response to ensure it meets the user's requirements or to improve the quality of the answer.\n",
    "reflect_prompt = ChatPromptTemplate.from_messages( # {\"tool_calls\", \"messages\"}\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"\"\" You are a debugging expert with expertise in Python, Langchain, and openai library. \n",
    "            \\nThe following tool_calls failed during runtime.\n",
    "            \\nTool Calls\n",
    "            \\n\\n```\n",
    "            \\n{tool_calls}\n",
    "            \\n\\n```\n",
    "            The error messages are listed as follows:\"\"\",\n",
    "        ),\n",
    "        (\"placeholder\", \"{messages}\"),\n",
    "        (\"system\", \"Provide the solution to the errors to aid in debugging.\")\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ee681ef",
   "metadata": {},
   "source": [
    "## State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a9668969-d755-431a-8281-688d4707f1b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphState(TypedDict):\n",
    "    task: str\n",
    "    tool_calls: list\n",
    "    iterations: int\n",
    "    reflections: str\n",
    "    error: str\n",
    "    messages: List\n",
    "    answer: str\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fb9f23a",
   "metadata": {},
   "source": [
    "## Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "48cde115-0aa7-4972-8149-78b61e78c845",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "# Modifying fix_tool_args_prompt may require changes to parse_arg.\n",
    "def parse_arg(ai_msg, arg):\n",
    "    pattern_string = f'{arg}=([^\\s,)]+)'\n",
    "    regex = re.compile(pattern_string) \n",
    "    # pattern = r'location=([^\\s,)]+)'\n",
    "    # match = re.search(pattern, ai_msg.content)\n",
    "    match = regex.search(ai_msg.content)\n",
    "    if match:\n",
    "        try:\n",
    "            return eval(match.group(1))\n",
    "        except Exception as e:\n",
    "            return match.group(1) or \"\"\n",
    "\n",
    "    \n",
    "class ToolLlm:\n",
    "    def __init__(self, llm, available_tools={}, max_iterations=5, reflect=False):\n",
    "        self.compiled = False\n",
    "        self.workflow = None\n",
    "        self.available_tools = available_tools\n",
    "        self.tools = []\n",
    "        for k in available_tools:\n",
    "            self.tools.append(available_tools[k])\n",
    "        self.llm = llm\n",
    "        self.max_iterations = max_iterations\n",
    "        self.llm_with_tools = llm.bind_tools(self.tools)\n",
    "        self.reflect = reflect\n",
    "        # self.retry_cahin = retry_prompt | self.llm # {\"task\", \"reflections\", \"tool_calls\"}\n",
    "        self.finish_chain = finish_prompt | self.llm # {\"task\", \"tool_outputs\"}\n",
    "        self.finish_wo_calls_chain = finish_wo_calls_prompt | self.llm # {\"task\"}\n",
    "        self.reflect_chain = reflect_prompt | self.llm # {\"tool_calls\", \"messages\"}\n",
    "        self.fix_tool_args_chain =  fix_tool_args_prompt | self.llm # {\"task\", \"tool_name\", \"reflections\", \"arg\", \"tool_description\", args}\n",
    "        self.workflow = StateGraph(GraphState)\n",
    "\n",
    "    def invoke(self, task):\n",
    "        if not self.compiled:\n",
    "            app = self.compile()\n",
    "        else:\n",
    "            app = self.workflow\n",
    "        return app.invoke({\"task\": [(\"user\", task)], \"iterations\": 0, \"messages\":[]})\n",
    "\n",
    "    def compile(self):\n",
    "        workflow = self.workflow\n",
    "        workflow.add_node(\"find_tool_node\", self.find_tool_node)\n",
    "        workflow.add_node(\"fix_tool_args_node\", self.fix_tool_args_node)\n",
    "        workflow.add_node(\"check_code_node\", self.check_code_node)\n",
    "        workflow.add_node(\"reflect_node\", self.reflect_node)\n",
    "        workflow.add_node(\"finish_node\", self.finish_node)\n",
    "        \n",
    "        # Build graph\n",
    "        workflow.add_edge(START, \"find_tool_node\")\n",
    "        workflow.add_conditional_edges(\n",
    "            \"find_tool_node\",\n",
    "            self.decide_to_finish,\n",
    "            {\n",
    "                \"finish\": \"finish_node\",\n",
    "                \"check\": \"check_code_node\",\n",
    "            },\n",
    "        )\n",
    "        workflow.add_conditional_edges(\n",
    "            \"check_code_node\",\n",
    "            self.decide_to_reflect,\n",
    "            {\n",
    "                \"finish\": \"finish_node\",\n",
    "                \"reflect\": \"reflect_node\",\n",
    "                \"retry\": \"find_tool_node\"\n",
    "            },\n",
    "        )\n",
    "        workflow.add_edge(\"finish_node\", END)\n",
    "        workflow.add_edge(\"reflect_node\", \"fix_tool_args_node\")\n",
    "        workflow.add_edge(\"fix_tool_args_node\", \"check_code_node\")\n",
    "        self.workflow = workflow.compile()\n",
    "        self.compiled = True\n",
    "        return self.workflow\n",
    "\n",
    "    def finish_node(self, state: GraphState):\n",
    "        \"\"\"\n",
    "        Find suitable tool to solve the problem\n",
    "    \n",
    "        Args:\n",
    "            state (dict): The current graph state\n",
    "    \n",
    "        Returns:\n",
    "            state (dict): New key added to state, generation\n",
    "        \"\"\"\n",
    "        # State\n",
    "        task = state[\"task\"]\n",
    "        tool_calls = state[\"tool_calls\"]\n",
    "        messages = state[\"messages\"]\n",
    "        iterations = state[\"iterations\"]\n",
    "        if tool_calls:\n",
    "            tool_outputs = {}\n",
    "            for tool in tool_calls:\n",
    "                tool_outputs[tool['name']] = tool['output']\n",
    "    \n",
    "            ai_msg = self.finish_chain.invoke({\"task\":task, \"tool_outputs\": tool_outputs})\n",
    "            # answer = ai_msg.content\n",
    "            # messages += [(\"system\", f\"{answer}\")]\n",
    "            # return {**state, \"tool_calls\": tool_calls, \"iterations\": iterations, \"messages\": messages, \"answer\": answer}\n",
    "        else:\n",
    "            ai_msg = self.finish_wo_calls_chain.invoke({\"task\":task})\n",
    "\n",
    "        answer = ai_msg.content\n",
    "        messages += [(\"system\", f\"{answer}\")]\n",
    "        return {**state, \"tool_calls\": tool_calls, \"iterations\": iterations, \"messages\": messages, \"answer\": answer}\n",
    "\n",
    "\n",
    "\n",
    "    def find_tool_node(self, state: GraphState):\n",
    "        \"\"\"\n",
    "        Find suitable tool to solve the problem\n",
    "    \n",
    "        Args:\n",
    "            state (dict): The current graph state\n",
    "    \n",
    "        Returns:\n",
    "            state (dict): New key added to state, generation\n",
    "        \"\"\"\n",
    "    \n",
    "        # print(\"---FINDING TOOL---\")\n",
    "    \n",
    "        # State\n",
    "        task = state[\"task\"]\n",
    "        iterations = state[\"iterations\"]\n",
    "        reflections = state[\"reflections\"]\n",
    "        error = state[\"error\"]\n",
    "        messages = state[\"messages\"]\n",
    "    \n",
    "        # Increment\n",
    "        iterations = iterations + 1\n",
    "        if error == 'yes':\n",
    "            # got error messages at check_code_node\n",
    "            task = [messages[-1]] + [('user', f'Try again to complete the task: {task[0][1]}')]\n",
    "        ai_msg = self.llm_with_tools.invoke(task)\n",
    "        tool_calls = getattr(ai_msg,'tool_calls', False)\n",
    "        if tool_calls:\n",
    "            return {**state, \"tool_calls\": tool_calls, \"iterations\": iterations, \"messages\": messages}\n",
    "        else:\n",
    "            return {**state, \"tool_calls\": [], \"iterations\": iterations, \"messages\": messages}\n",
    "\n",
    "    \n",
    "    def check_code_node(self, state: GraphState):\n",
    "        \"\"\"\n",
    "        Check code\n",
    "    \n",
    "        Args:\n",
    "            state (dict): The current graph state\n",
    "    \n",
    "        Returns:\n",
    "            state (dict): New key added to state, error\n",
    "        \"\"\"\n",
    "    \n",
    "        # print(\"---CHECKING CODE---\")\n",
    "        \n",
    "        \n",
    "        # State\n",
    "        task = state[\"task\"]\n",
    "        tool_calls = state[\"tool_calls\"]\n",
    "    \n",
    "        # Check execution\n",
    "        try:\n",
    "            for i in range(len(tool_calls)):\n",
    "                tool = tool_calls[i]\n",
    "                tool['output'] = self.available_tools[tool['name']].invoke(tool['args'])\n",
    "                tool_calls[i] = tool\n",
    "        except Exception as e:\n",
    "            print(\"---TOOL EXECUTION: FAILED---\")\n",
    "            error_message = [(\"user\", f\"Your solution failed the test: {e}\")]\n",
    "            messages += error_message\n",
    "            return {\n",
    "                **state,\n",
    "                \"messages\": messages,\n",
    "                \"error\": \"yes\",\n",
    "            }\n",
    "    \n",
    "        # No errors\n",
    "        # print(\"---NO CODE TEST FAILURES---\")\n",
    "        return {\n",
    "            **state,\n",
    "            \"error\": \"no\",\n",
    "            \"tool_calls\": tool_calls\n",
    "        }\n",
    "\n",
    "    def reflect_node(self, state: GraphState):\n",
    "        \"\"\"\n",
    "        Reflect on errors\n",
    "    \n",
    "        Args:\n",
    "            state (dict): The current graph state\n",
    "    \n",
    "        Returns:\n",
    "            state (dict): New key added to state, generation\n",
    "        \"\"\"\n",
    "    \n",
    "        # print(\"---GENERATING CODE SOLUTION---\")\n",
    "    \n",
    "        # State\n",
    "        # task = state[\"task\"]\n",
    "        tool_calls = state[\"tool_calls\"]\n",
    "    \n",
    "    \n",
    "        # Add reflection\n",
    "        reflections = self.reflect_chain.invoke(\n",
    "            {\"tool_calls\": tool_calls, \"messages\": messages}\n",
    "        ).content\n",
    "        messages += [(\"assistant\", f\"Here are reflections on the error: {reflections}\")]\n",
    "        return {**state, \"reflections\": reflections, \"messages\": messages}\n",
    "\n",
    "\n",
    "    def fix_tool_args_node(self, state: GraphState):\n",
    "        \"\"\"\n",
    "        Fix the parameters for the selected tool\n",
    "    \n",
    "        Args:\n",
    "            state (dict): The current graph state\n",
    "    \n",
    "        Returns:\n",
    "            state (dict): New key added to state, generation\n",
    "        \"\"\"\n",
    "    \n",
    "        # print(\"---FINDING TOOL ARGS---\")\n",
    "        \n",
    "        # State\n",
    "        task = state[\"task\"]\n",
    "        tool_calls = state[\"tool_calls\"]\n",
    "        iterations = state[\"iterations\"]\n",
    "        iterations += 1\n",
    "        reflections = state[\"reflections\"]\n",
    "        error = state[\"error\"]\n",
    "        messages = state[\"messages\"]\n",
    "        for i in range(len(tool_calls)):\n",
    "            tool = tool_calls[i]\n",
    "            fixed_args = {}\n",
    "            for arg in tool.args:\n",
    "                # {\"tool_name\", \"reflections\", \"arg\", \"tool_description\", args}\n",
    "                ai_msg = self.fix_tool_args_chain.invoke({\"tool_name\": tool['name'], \"reflections\": reflections,\n",
    "                                                 \"arg\": arg, \"tool_description\": tool.description, \"arg_info\": json.dumps(tool.args[arg])})\n",
    "                fixed_args[arg] = parse_arg(ai_msg, arg)\n",
    "                \n",
    "            tool['args'] = fixed_args\n",
    "            tool_calls[i] = tool\n",
    "                \n",
    "        return {**state, \"tool_calls\": tool_calls, \"iterations\": iterations, \"messages\": messages}\n",
    "\n",
    "    ### Edges\n",
    "\n",
    "    def decide_to_finish(self, state: GraphState):\n",
    "        \"\"\"\n",
    "        Determines whether to finish.\n",
    "    \n",
    "        Args:\n",
    "            state (dict): The current graph state\n",
    "    \n",
    "        Returns:\n",
    "            str: Next node to call\n",
    "        \"\"\"\n",
    "        tool_calls = state[\"tool_calls\"]\n",
    "    \n",
    "        if tool_calls:\n",
    "            print(\"---DECISION: CHECK CODE---\")\n",
    "            return \"check\"\n",
    "        else:\n",
    "            print(\"---DECISION: FINISH---\")\n",
    "            return \"finish\"\n",
    "    \n",
    "    \n",
    "    def decide_to_reflect(self, state: GraphState):\n",
    "        \"\"\"\n",
    "        Determines whether to reflect.\n",
    "    \n",
    "        Args:\n",
    "            state (dict): The current graph state\n",
    "    \n",
    "        Returns:\n",
    "            str: Next node to call\n",
    "        \"\"\"\n",
    "\n",
    "        error = state[\"error\"]\n",
    "        iterations = state[\"iterations\"]\n",
    "    \n",
    "        if error == \"no\" or iterations == self.max_iterations:\n",
    "            print(\"---DECISION: FINISH---\")\n",
    "            return \"finish\"\n",
    "        else:\n",
    "            if self.reflect == \"reflect\":\n",
    "                print(\"---DECISION: RE-TRY SOLUTION---\")\n",
    "                return \"reflect\"\n",
    "            else:\n",
    "                return \"retry\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "491e370b-9aff-451e-a025-6db9457ebc36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---DECISION: CHECK CODE---\n",
      "---DECISION: FINISH---\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'task': [('user', 'give the weather in nyc')],\n",
       " 'tool_calls': [{'name': 'get_weather',\n",
       "   'args': {'city': 'nyc'},\n",
       "   'id': 'call_b730c3cdf6ff42568a15276f62906c4b',\n",
       "   'type': 'tool_call',\n",
       "   'output': 'cloudy, 30 degrees celcius'}],\n",
       " 'iterations': 1,\n",
       " 'reflections': None,\n",
       " 'error': 'no',\n",
       " 'messages': [('system',\n",
       "   \"Unfortunately, I don't have access to real-time weather information. However, based on our previous conversation where we used tool 'get_weather', it mentioned that the weather is cloudy with a temperature of 30 degrees Celsius. Please note that this might not be up-to-date or specific to NYC.\")],\n",
       " 'answer': \"Unfortunately, I don't have access to real-time weather information. However, based on our previous conversation where we used tool 'get_weather', it mentioned that the weather is cloudy with a temperature of 30 degrees Celsius. Please note that this might not be up-to-date or specific to NYC.\"}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "app = ToolLlm(llm_function, available_tools=available_tools)\n",
    "app.invoke(\"give the weather in nyc\")\n",
    "# app = app.compile()\n",
    "# app.invoke({\"task\": [(\"user\", \"give the weather in nyc\")], \"iterations\": 0, \"messages\":[]})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
