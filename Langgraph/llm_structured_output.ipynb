{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b10d3800",
   "metadata": {},
   "source": [
    "# LLM Structured Output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fe1c0ec",
   "metadata": {},
   "source": [
    "StructuredOutputLlm is designed for robust output with structure functionality. If an error occurs during the execution of a tool, the LLM model will be asked to carefully examine each generated argument and attempt to fix the problem.\n",
    "\n",
    "However, reflection is computationally expensive. It is recommended to make descriptions clearer in the Pydantic model rather than relying on reflection for fixing issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8723a4fd-8887-4b96-9e5a-ee7b6b9cb336",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from langchain import hub\n",
    "from langchain_experimental.llms.ollama_functions import OllamaFunctions\n",
    "# from langgraph.prebuilt import create_react_agent\n",
    "from langchain.globals import set_verbose, set_debug\n",
    "from langchain.agents import AgentExecutor, create_react_agent\n",
    "from langgraph.constants import Send\n",
    "from langgraph.graph import END, StateGraph, START\n",
    "from langchain.globals import set_verbose, set_debug\n",
    "from typing import List, TypedDict, Literal\n",
    "import json\n",
    "from langchain_core.tools import tool, StructuredTool\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "from langchain_experimental.llms.ollama_functions import OllamaFunctions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c8f3bff2-9070-4aeb-8d94-a8f602c6906f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.chat_models import ChatOllama\n",
    "llm = ChatOllama(model=\"llama3.1\", temperature=0.1)\n",
    "\n",
    "llm_function = OllamaFunctions(model=\"llama3.1\", format=\"json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "05f960eb-fc5e-40d7-bf71-442685da719b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fix_structure_output_prompt = ChatPromptTemplate.from_messages( # {\"tool_name\": tool.name, \"reflections\": reflections,\n",
    "                                          # \"arg\": arg, \"tool_description\": tool.description, \"arg_info\": json.dumps(tool.args[arg])}\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"\"\"You are an AI assistant equipped with various tools to help answer questions and solve problems. \n",
    "            \\nYou have tried to use {tool_name}, but errors occurred. The reflections and tool informations are shown below:\n",
    "            \\nTool Description:\n",
    "            \\n\\n```\n",
    "            \\n{tool_description}\n",
    "            \\n```\n",
    "            \\n\n",
    "            \\nTool Args Schema:\n",
    "            \\n\\n```\n",
    "            \\n{args}\n",
    "            \\n```\n",
    "            \\n\n",
    "            \\nReflections:\n",
    "            \\n\\n```\n",
    "            \\n{reflections}\n",
    "            \\n```\n",
    "            \\n\n",
    "            \\nWhat is correct {arg} parameter needed to resolve the issue? Only give the best one.\n",
    "            \\nThe answer should follow the following format.\n",
    "            \\n\\nCorrect function call:\n",
    "            \\n\\n```\n",
    "            \\nmy_function(arg_1=fixed_arg1, arg_2=fixed_arg2)\n",
    "            \\n```\n",
    "            \\n\n",
    "            \\n[INSERT YOUR ANSWER HERE]\n",
    "            \"\"\",\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "reflect_prompt = ChatPromptTemplate.from_messages( # {\"task\": task, \"schema\": schema, \"messages\": messages}\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"\"\" You are a debugging expert with expertise in Python and Pydantic.\n",
    "            \\nAn agent tried to convert a description into an output with structured format:\n",
    "            \\nDescription:\n",
    "            \\n\\n```\n",
    "            \\n{task}\n",
    "            \\n\\n```\n",
    "            \\nStructured Format:\n",
    "            \\n\\n```\n",
    "            \\n{schema}\n",
    "            \\n\\n```\n",
    "            \\nThe error messages received by the agent are listed as follows:\n",
    "            \\n {messages}\n",
    "            \\n\\n Provide reflections on these errors and the corrected structured output.\n",
    "            \\n[YOUR ANSWER]\"\"\",\n",
    "        ),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a9668969-d755-431a-8281-688d4707f1b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphState(TypedDict):\n",
    "    task: str\n",
    "    structured_output: dict\n",
    "    iterations: int\n",
    "    reflections: str\n",
    "    error: str\n",
    "    messages: List\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "48cde115-0aa7-4972-8149-78b61e78c845",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from pydantic import ValidationError    \n",
    "    \n",
    "\n",
    "def parse_arg(ai_msg, arg):\n",
    "    pattern_string = f'{arg}=([^\\s,)]+)'\n",
    "    regex = re.compile(pattern_string) \n",
    "    # pattern = r'location=([^\\s,)]+)'\n",
    "    # match = re.search(pattern, ai_msg.content)\n",
    "    match = regex.search(ai_msg.content)\n",
    "    if match:\n",
    "        try:\n",
    "            return eval(match.group(1))\n",
    "        except Exception as e:\n",
    "            return match.group(1) or \"\"\n",
    "\n",
    "    \n",
    "class StructuredOutputLlm:\n",
    "    def __init__(self, llm, structured_model, max_iterations=5, reflect=False):\n",
    "        self.structured_model = structured_model\n",
    "        self.compiled = False\n",
    "        self.workflow = None\n",
    "        self.llm = llm\n",
    "        self.max_iterations = max_iterations\n",
    "        self.llm_with_structured_output = llm.with_structured_output(structured_model)\n",
    "        self.reflect = reflect\n",
    "        self.reflect_prompt = reflect_prompt\n",
    "        self.reflect_chain = reflect_prompt | self.llm # {\"task\": task, \"structured_output\": structured_output, \"messages\": messages}\n",
    "        self.fix_structure_output_chain =  fix_structure_output_prompt | self.llm # {\"task\", \"tool_name\", \"reflections\", \"arg\", \"tool_description\", args}\n",
    "\n",
    "        # self.workflow = StateGraph(GraphState)\n",
    "        self.structured_output_tool = StructuredTool.from_function(\n",
    "            func=self.structured_model,\n",
    "            name=\"StructuredOutput\",\n",
    "            description=\"Return args in pydantic way\",\n",
    "            # coroutine= ... <- you can specify an async method if desired as well\n",
    "        )\n",
    "\n",
    "    def invoke(self, task):\n",
    "        if not self.compiled:\n",
    "            app = self.compile()\n",
    "        else:\n",
    "            app = self.workflow\n",
    "        return app.invoke({\"task\": [(\"user\", task)], \"iterations\": 0, \"messages\":[]})\n",
    "            \n",
    "    def compile(self):\n",
    "        workflow = StateGraph(GraphState)\n",
    "        workflow.add_node(\"create_structure_output_node\", self.create_structure_output_node)\n",
    "        workflow.add_node(\"fix_structure_output_node\", self.fix_structure_output_node)\n",
    "        workflow.add_node(\"reflect_node\", self.reflect_node)\n",
    "        \n",
    "        # Build graph\n",
    "        workflow.add_edge(START, \"create_structure_output_node\")        \n",
    "        workflow.add_conditional_edges(\n",
    "            \"create_structure_output_node\",\n",
    "            self.decide_to_reflect,\n",
    "            {\n",
    "                \"finish\": END,\n",
    "                \"reflect\": \"reflect_node\",\n",
    "                \"retry\": \"create_structure_output_node\"\n",
    "            },\n",
    "        )\n",
    "        workflow.add_edge(\"reflect_node\", \"fix_structure_output_node\")\n",
    "        workflow.add_conditional_edges(\n",
    "            \"fix_structure_output_node\",\n",
    "            self.decide_to_reflect,\n",
    "            {\n",
    "                \"finish\": END,\n",
    "                \"reflect\": \"reflect_node\",\n",
    "            },\n",
    "        )\n",
    "        self.workflow = workflow.compile()\n",
    "        self.compiled = True\n",
    "        return self.workflow\n",
    "\n",
    "    def create_structure_output_node(self, state: GraphState):\n",
    "        \"\"\"\n",
    "        Find suitable tool to solve the problem\n",
    "    \n",
    "        Args:\n",
    "            state (dict): The current graph state\n",
    "    \n",
    "        Returns:\n",
    "            state (dict): New key added to state, generation\n",
    "        \"\"\"\n",
    "    \n",
    "        # State\n",
    "        task = state[\"task\"]\n",
    "        structured_output = state[\"structured_output\"]\n",
    "        messages = state[\"messages\"]\n",
    "        iterations = state[\"iterations\"]\n",
    "        iterations += 1        \n",
    "        error = state[\"error\"]\n",
    "        structured_output = None\n",
    "        try:\n",
    "            if error == 'yes':\n",
    "                task = [messages[-1]] + [('user', f'Try again to complete the task: {task[0][1]}')]\n",
    "            structured_output = self.structured_model.validate(self.llm_with_structured_output.invoke(task))\n",
    "            error = 'no'\n",
    "        except Exception as e:\n",
    "            print(f'Error occured: {e}')\n",
    "            messages += [(\"system\", f\"Errors occured in your last attempt: {e}\")]\n",
    "            error = 'yes'\n",
    "        \n",
    "        return {**state, \"error\": error, \"iterations\": iterations, \"messages\": messages, \"structured_output\": structured_output}\n",
    "\n",
    "    \n",
    "\n",
    "    def reflect_node(self, state: GraphState):\n",
    "        \"\"\"\n",
    "        Reflect on errors\n",
    "    \n",
    "        Args:\n",
    "            state (dict): The current graph state\n",
    "    \n",
    "        Returns:\n",
    "            state (dict): New key added to state, generation\n",
    "        \"\"\"\n",
    "    \n",
    "        print(\"---GENERATING CODE SOLUTION---\")\n",
    "    \n",
    "        # State\n",
    "        task = state[\"task\"]\n",
    "        structured_output = state[\"structured_output\"]\n",
    "        messages = state[\"messages\"]\n",
    "        schema = schema_json_of(self.structured_model)\n",
    "\n",
    "    \n",
    "        # Add reflection\n",
    "        err_msg = self.reflect_prompt.invoke({\"task\": task, \"messages\": messages, \"schema\": schema})\n",
    "        reflections = self.llm.invoke(\n",
    "            err_msg[0].content\n",
    "        ).content\n",
    "        messages += [(\"assistant\", f\"Here are reflections on the error: {reflections}\")]\n",
    "        return {**state, \"reflections\": reflections, \"messages\": messages}\n",
    "\n",
    "    def fix_structure_output_node(self, state: GraphState):\n",
    "        \"\"\"\n",
    "        Fix the parameters for the selected tool\n",
    "    \n",
    "        Args:\n",
    "            state (dict): The current graph state\n",
    "    \n",
    "        Returns:\n",
    "            state (dict): New key added to state, generation\n",
    "        \"\"\"\n",
    "    \n",
    "        print(\"---FINDING TOOL ARGS---\")\n",
    "        \n",
    "        # State\n",
    "        task = state[\"task\"]\n",
    "        iterations = state[\"iterations\"]\n",
    "        iterations += 1\n",
    "        reflections = state[\"reflections\"]\n",
    "        error = state[\"error\"]\n",
    "        messages = state[\"messages\"]\n",
    "        \n",
    "        tool = self.structured_output_tool\n",
    "        fixed_args = {}\n",
    "        for arg in tool.args:\n",
    "        # {\"tool_name\", \"reflections\", \"arg\", \"tool_description\", args}\n",
    "            ai_msg = self.fix_structure_output_chain.invoke({\"tool_name\": tool.name, \"reflections\": reflections,\n",
    "                                          \"arg\": arg, \"tool_description\": tool.description, \"arg_info\": json.dumps(tool.args[arg])})\n",
    "            fixed_args[arg] = parse_arg(ai_msg, arg)\n",
    "\n",
    "        try:\n",
    "            fixed_structured_model = self.structured_model(**fixed_args)\n",
    "            fixed_structured_model= self.structured_model.validate(fixed_structured_model)\n",
    "            error = 'no'\n",
    "        except ValidationError as e:\n",
    "            messages += [(\"system\", f\"{e}\")]\n",
    "            error = 'yes'\n",
    "\n",
    "        return {**state, \"error\": error, \"iterations\": iterations, \"messages\": messages, \"structured_output\": fixed_structured_model}\n",
    "        \n",
    "    ### Edges\n",
    "    \n",
    "    def decide_to_reflect(self, state: GraphState):\n",
    "        \"\"\"\n",
    "        Determines whether to reflect.\n",
    "    \n",
    "        Args:\n",
    "            state (dict): The current graph state\n",
    "    \n",
    "        Returns:\n",
    "            str: Next node to call\n",
    "        \"\"\"\n",
    "\n",
    "        error = state[\"error\"]        \n",
    "        iterations = state[\"iterations\"]\n",
    "        if error == \"no\" or iterations == self.max_iterations:\n",
    "            print(\"---DECISION: FINISH---\")\n",
    "            return \"finish\"\n",
    "        else:\n",
    "            if self.reflect:\n",
    "                print(\"---DECISION: REFLECTING SOLUTION---\")\n",
    "                return \"reflect\"\n",
    "            else:\n",
    "                print(f\"---DECISION: RETRY (times:{iterations})---\")\n",
    "                return \"retry\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0dcf4dea-ccb1-4460-ab76-aad6b4ae2ef4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Joke(setup='Why did the cat join a band?', punchline='Because it wanted to be the purr-cussionist!')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.pydantic_v1 import BaseModel, Field, schema_json_of\n",
    "from typing import Literal, Union\n",
    "\n",
    "from typing import Annotated\n",
    "\n",
    "# from pydantic import BaseModel, Field, schema_json_of,  TypeAdapter\n",
    "\n",
    "class Joke(BaseModel):\n",
    "    setup: str = Field(description=\"The setup of the joke\")\n",
    "    punchline: str = Field(description=\"The punchline to the joke\")\n",
    "    \n",
    "structured_llm = llm_function.with_structured_output(Joke)\n",
    "structured_llm.invoke(\"Tell me a joke about cats\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "491e370b-9aff-451e-a025-6db9457ebc36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---DECISION: FINISH---\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'task': [('user', 'give a joke about dog')],\n",
       " 'structured_output': Joke(setup='Why did the dog go to the vet?', punchline='Because he was feeling a little ruff!'),\n",
       " 'iterations': 1,\n",
       " 'reflections': None,\n",
       " 'error': 'no',\n",
       " 'messages': []}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "app = StructuredOutputLlm(llm_function, structured_model=Joke)\n",
    "app.invoke(\"give a joke about dog\")\n",
    "# app = app.compile()\n",
    "# app.invoke({\"task\": [(\"user\", \"give a joke about cat\")], \"iterations\": 0, \"messages\":[]})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee79335d",
   "metadata": {},
   "source": [
    "## Reflections on errors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "728cb4cd",
   "metadata": {},
   "source": [
    "Let's examine how the LLM handles a missing field input through reflection. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4be8f581-d586-4f06-b560-ab1086461887",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " You are a debugging expert with expertise in Python and Pydantic.\n",
      "            \n",
      "An agent tried to convert a description into an output with structured format:\n",
      "            \n",
      "Description:\n",
      "            \n",
      "\n",
      "```\n",
      "            \n",
      "[('user', 'give a joke about dog')]\n",
      "            \n",
      "\n",
      "```\n",
      "            \n",
      "Structured Format:\n",
      "            \n",
      "\n",
      "```\n",
      "            \n",
      "{\"title\": \"ParsingModel[Joke]\", \"$ref\": \"#/definitions/Joke\", \"definitions\": {\"Joke\": {\"title\": \"Joke\", \"type\": \"object\", \"properties\": {\"setup\": {\"title\": \"Setup\", \"description\": \"The setup of the joke\", \"type\": \"string\"}, \"punchline\": {\"title\": \"Punchline\", \"description\": \"The punchline to the joke\", \"type\": \"string\"}}, \"required\": [\"setup\", \"punchline\"]}}}\n",
      "            \n",
      "\n",
      "```\n",
      "            \n",
      "The error messages received by the agent are listed as follows:\n",
      "            \n",
      " [('assistant', 'ValidationError: 1 validation error for Joke punchline field required (type=value_error.missing)')]\n",
      "            \n",
      "\n",
      " Provide reflections on these errors and the corrected structured output.\n",
      "            \n",
      "[YOUR ANSWER]\n"
     ]
    }
   ],
   "source": [
    "# {\"task\": task, \"schema\": schema, \"messages\": messages}\n",
    "err_msg = reflect_prompt.invoke({\"task\": [(\"user\", \"give a joke about dog\")],\n",
    "                            \"schema\":schema_json_of(Joke), \n",
    "                            \"messages\": [(\"assistant\", \"ValidationError: 1 validation error for Joke punchline field required (type=value_error.missing)\")]}).messages[0].content\n",
    "\n",
    "print(err_msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "617fb930-d277-4c03-ac16-3b1770a60e01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the provided description and structured format, it seems that the agent is trying to convert a natural language input into a structured output using Pydantic models. However, there's an error in the conversion process.\n",
      "\n",
      "The error message `ValidationError: 1 validation error for Joke punchline field required (type=value_error.missing)` indicates that the \"punchline\" field in the Joke model is missing a value. This suggests that the agent failed to extract or parse the punchline from the input description.\n",
      "\n",
      "Let's analyze the input description `[('user', 'give a joke about dog')]`. It seems that this description is asking for a joke about dogs, but it doesn't provide any specific setup or punchline.\n",
      "\n",
      "Given this context, I would reflect on the following:\n",
      "\n",
      "1. **Missing required fields**: The Joke model requires both \"setup\" and \"punchline\" fields to be present. However, in this case, only the \"setup\" field is implicitly provided (i.e., \"give a joke about dog\"). The agent should either extract or generate a punchline to fulfill the required field.\n",
      "2. **Incomplete input**: The input description might not contain enough information to create a complete Joke model. In such cases, the agent could consider asking for additional context or clarification from the user.\n",
      "\n",
      "To correct the structured output, I would suggest adding a \"punchline\" field with an empty string value (or a default value if applicable). This would ensure that the Joke model is properly formatted and can be used for further processing.\n",
      "\n",
      "Here's an example of the corrected structured output:\n",
      "```json\n",
      "{\n",
      "    \"title\": \"ParsingModel[Joke]\",\n",
      "    \"$ref\": \"#/definitions/Joke\",\n",
      "    \"definitions\": {\n",
      "        \"Joke\": {\n",
      "            \"title\": \"Joke\",\n",
      "            \"type\": \"object\",\n",
      "            \"properties\": {\n",
      "                \"setup\": {\"title\": \"Setup\", \"description\": \"The setup of the joke\", \"type\": \"string\"},\n",
      "                \"punchline\": {\"title\": \"Punchline\", \"description\": \"The punchline to the joke\", \"type\": \"string\"}\n",
      "            },\n",
      "            \"required\": [\"setup\", \"punchline\"]\n",
      "        }\n",
      "    },\n",
      "    \"Joke\": {\n",
      "        \"setup\": \"give a joke about dog\",\n",
      "        \"punchline\": \"\"  # corrected output with empty string value\n",
      "    }\n",
      "}\n",
      "```\n",
      "Note that this is just one possible correction, and the actual solution might depend on the specific requirements of your application.\n"
     ]
    }
   ],
   "source": [
    "ai_msg = llm.invoke(err_msg)\n",
    "print(ai_msg.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dd6c89c",
   "metadata": {},
   "source": [
    "The reflection gives the correct structured output but it is computational expensive. Please use this feature with cautious.\n",
    "It is always recommended to first make descriptions clearer in the Pydantic model rather than relying on reflection for fixing issues."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
